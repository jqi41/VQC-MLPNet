#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb  6 18:31:52 2025

@author: junqi
"""

"""Charge stability diagram classification with MLP-MPS-VQC.

Dataset: the configuration are generated by our simulator on a 50x50 square lattice for 
both noisy and noiseless cases.""" 

# Helper Libraries 
import numpy as np
import matplotlib.pyplot as plt
import argparse

# Machine learning related libraries:
import torch 
import torch.nn as nn

import torchquantum as tq

# DataLoader utilities
from torch.utils.data import DataLoader  
from torch.utils.data import Dataset     

# Import the modified hybrid quantumâ€“classical models that include depolarizing noise
from mlp_vqc import MLP_VQC
from mlp_mps_vqc import MLP_MPS_VQC
from mlp_tree_vqc import MLP_TREE_VQC 

seed = 1234
torch.manual_seed(seed)

parser = argparse.ArgumentParser(
    description='Training a hybrid quantum-classical model (MLP_VQC variants) for charge stability diagram classification of quantum dots with depolarizing noise simulation.'
)
parser.add_argument('--save_path', metavar='DIR', default='models', help='Path to save the trained model')
parser.add_argument('--num_qubits', default=20, help='Number of qubits in the quantum circuit', type=int)
parser.add_argument('--hidden_units', default=64, help='Number of hidden units', type=int)
parser.add_argument('--batch_size', default=8, help='Batch size for training', type=int)
parser.add_argument('--num_epochs', default=20, help='Number of training epochs', type=int)
parser.add_argument('--depth_vqc', default=6, help='Depth (number of variational layers) of the VQC', type=int)
parser.add_argument('--lr', default=0.01, help='Learning rate', type=float)
parser.add_argument('--test_kind', metavar='DIR', default='gen', help='Test type: "rep" for representation, "gen" for generalization')
parser.add_argument('--model_kind', metavar='DIR', default='vqc', help='Model type: vqc, mps_vqc, tree_vqc')
parser.add_argument('--amplitude_damping_rate', default=0.05, help='amplitude_damping_rate', type=float)
parser.add_argument('--phase_damping_rate', default=0.05, help='phase_damping_rate', type=float)

args = parser.parse_args()

####### Detect if running on a GPU/CPU #######
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    print("is CUDA available?", torch.cuda.is_available())
else:
    device = torch.device("cpu")
    print("Running on the CPU")
    
# Load datasets
with open('mlqe_2023_edx/week1/dataset/csds.npy', 'rb') as f:
    data_noisy = np.load(f)

with open('mlqe_2023_edx/week1/dataset/csds_noiseless.npy', 'rb') as f:
    data_clean = np.load(f)
    
with open('mlqe_2023_edx/week1/dataset/labels.npy', 'rb') as f:
    labels = np.load(f)

# Visualize 10 random noiseless charge stability diagrams with their labels
fig, ax = plt.subplots(1, 10, figsize=(20, 10))
for index, d in enumerate(data_clean[np.random.choice(len(data_clean), size=10)]):
    ax[index].imshow(d)
    ax[index].axis('off')
    ax[index].set_title(f'Label: {labels[index]}')
plt.show()
plt.close()


""" Data preparation """
class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = torch.Tensor(data)
        self.labels = torch.Tensor(labels)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        data_idx = self.data[idx]
        label = self.labels[idx].type(torch.LongTensor)
        return data_idx, label
    
# Use the noisy dataset for generalization test, otherwise use the clean dataset.
if args.test_kind == 'gen':
    dataset = CustomDataset(data_noisy, labels)
else:
    dataset = CustomDataset(data_clean, labels)
trainset, testset = torch.utils.data.random_split(
    dataset, (int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8))
)

batch_size = args.batch_size
trainloader = DataLoader(trainset, batch_size=batch_size)
testloader = DataLoader(testset, batch_size=batch_size)

# Check a sample batch shape
for X, y in trainloader:
    print(f"Shape of X: {X.shape}")
    print(f"Shape of y: {y.shape} {y.dtype}")
    print(y)
    break

# Flatten each sample and obtain input dimension
bsz, input_dims = (X.reshape(X.shape[0], -1)).shape

print(f"Using {device} device")
    
# Instantiate the model with depolarizing noise
if args.model_kind == "vqc":
    model = MLP_VQC(
        args.num_qubits, 
        args.depth_vqc, 
        input_dims=input_dims, 
        hidden_units=args.hidden_units, 
        amplitude_damping_rate = args.amplitude_damping_rate, 
        phase_damping_rate = args.phase_damping_rate,
    ).to(device)
elif args.model_kind == "mps_vqc":
    model = MLP_MPS_VQC(
        args.num_qubits, 
        args.depth_vqc, 
        input_dims=input_dims, 
        hidden_units=args.hidden_units, 
        amplitude_damping_rate = args.amplitude_damping_rate, 
        phase_damping_rate = args.phase_damping_rate,
    ).to(device)
elif args.model_kind == "tree_vqc":
    model = MLP_TREE_VQC(
        args.num_qubits, 
        args.depth_vqc, 
        input_dims=input_dims, 
        hidden_units=args.hidden_units, 
        amplitude_damping_rate = args.amplitude_damping_rate, 
        phase_damping_rate = args.phase_damping_rate,
    ).to(device)
else:
    raise Exception("Only vqc, mps_vqc, and tree_vqc are supported")

print(model)

# Print number of trainable parameters
pytorch_total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable params: {pytorch_total_params}")

# Define loss, optimizer, and learning rate scheduler
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# Lists to store training and test metrics
train_loss_model = []
train_acc_model = []
test_loss_model = []
test_acc_model = []

# Define the training function
def train(dataloader, dev, model, loss_fn, optimizer, train_loss, train_acc):
    num_batches = len(dataloader)
    size = len(dataloader.dataset)
    model.train()
    running_loss, correct = 0, 0
    W1 = None
    for batch, (X, y) in enumerate(dataloader):
        X = nn.Flatten()(X)
        X, y = X.to(device), y.to(device)
        
        pred, W1 = model(X, q_device=dev, W1=W1, is_train=True)
        loss = loss_fn(pred, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        correct += (pred.argmax(1) == y).type(torch.float).sum().item()
        
        if batch % 25 == 0:
            loss_val, current = loss.item(), batch * len(X)
            print(f"loss: {loss_val:>7f} [{current:>5d} / {size:>5d}]")
    running_loss /= num_batches
    correct /= size
    
    train_acc.append(correct)
    train_loss.append(running_loss)
    
    return W1

# Define the testing function
def test(dataloader, dev, model, loss_fn, test_loss, test_acc, W1):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    running_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X = nn.Flatten()(X)
            X, y = X.to(device), y.to(device)
            pred = model(X, q_device=dev, is_train=False, W1=W1)
            running_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    running_loss /= num_batches
    correct /= size
    
    test_acc.append(correct)
    test_loss.append(running_loss)
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {running_loss:>8f}\n")
    
# Function to plot accuracy and loss graphs
def create_acc_loss_graph(train_acc, train_loss, test_acc, test_loss):
    fig, axes = plt.subplots(ncols=2, nrows=1, dpi=300)
    fig.set_size_inches(9, 3)
    ax1, ax2 = axes[0], axes[1]
    
    ax1.plot(train_acc, '-o', label="train", markersize=4)
    ax1.plot(test_acc, '--+', label="test", markersize=4)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend(loc=3)
    
    ax2.plot(train_loss, '-o', label='train', markersize=4)
    ax2.plot(test_loss, '--+', label='test', markersize=4)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend(loc=1)
    
    ax1.set_ylim(0, np.max([np.max(train_acc), np.max(test_acc)]) + 0.1)
    ax2.set_ylim(-0.1, np.max([np.max(train_loss), np.max(test_loss)]) + 0.1)
    ax1.grid(True, which='both', linewidth=0.1)
    ax2.grid(True, which='both', linewidth=0.1)
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    # Create a quantum device with the specified number of qubits and batch size
    dev = tq.QuantumDevice(n_wires=args.num_qubits, bsz=bsz)
    for t in range(args.num_epochs):
        print(f"Epoch {t+1}\n --------------------")
        W1 = train(trainloader, dev, model, loss_fn, optimizer, train_loss_model, train_acc_model)
        test(testloader, dev, model, loss_fn, test_loss_model, test_acc_model, W1=W1)
    print("Done!")
    
    model.save_model("models/mlp_vqc.pt")
    create_acc_loss_graph(train_acc_model, train_loss_model, test_acc_model, test_loss_model)
